{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Synthetic NER data for fine-tuning GLiNER\n",
        "This notebook is intended to show you how to create a synthetic NER dataset using OpenAI's API. This example was developed by the *Congruence Engine* team with a view to creating a synthetic dataset for fine-tuning the NER model [GLiNER](https://github.com/urchade/GLiNER).\n",
        "\n",
        "In this example, we use a dataset of 2,517 textile terms extracted from four glossaries published in the US and United Kingdom in the late 19th and early 20th centuries. Prior to this, we classified the terms according to the following labels:\n",
        "* textile fabric\n",
        "* textile fabric component\n",
        "* textile fabric imperfection\n",
        "* textile fibre\n",
        "* textile manufacturing process\n",
        "* textile machinery\n",
        "* textile weave\n",
        "* textile manufacturing chemical\n",
        "* textile dye\n",
        "* textile industry expression\n",
        "* textile industry unit of measurement\n",
        "* textile waste material\n",
        "\n",
        "To run this code, you will need an OpenAI API Key. You can read the documentation for the API [here](https://platform.openai.com/docs/overview). It is also possible to run this code on other models and APIs, including by setting up a smaller model to run locally on your device via a programme like [LM Studio](https://lmstudio.ai/).\n",
        "\n",
        "To see how we used this data to fine-tune a GLiNER model, see [this notebook](https://github.com/congruence-engine/universal-ner-with-gliner/blob/main/code/GLiNER_finetune_notebook.ipynb). {You can find a [Colab version here](https://colab.research.google.com/drive/1j1tE2bi5qrWVBKyTEgwbMISXUOvasMbs?usp=sharing)}"
      ],
      "metadata": {
        "id": "O3MzRRJphQae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the required libraries**"
      ],
      "metadata": {
        "id": "pGDvEHwZYmyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import concurrent.futures\n",
        "import time\n",
        "import pandas as pd\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "vVWuhsrNYiwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticate with the OpenAI API**\n",
        "\n",
        "You will need an [OpenAI API](https://platform.openai.com/docs/overview) Key for this."
      ],
      "metadata": {
        "id": "QSdjLF7VYqgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
      ],
      "metadata": {
        "id": "nqnubi3kYpz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code below does the following:**\n",
        "* Establishes a standard prompt to send to the model, which includes:\n",
        "      1. An explanation of the task\n",
        "      2. A single term extracted from the dataset, along with its definition, and its entity label\n",
        "      3. Format requirements\n",
        "      4. An outsput schema in JSON format\n",
        "      5. A full example\n",
        "* Determines how many examples to generate from each term (here set to 3)\n",
        "* Iterates over each term in the dataframe, and passes the relevant data to the model\n",
        "* Ensures that the output is in valid json, and collects the sum of all responses into a single json output.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aka9KGGpbLaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvnby5GEYc4v"
      },
      "outputs": [],
      "source": [
        "def create_json_prompt_for_synthetic_data(term, definition, predicted_label, **kwargs):\n",
        "    attributes = {key: value for key, value in kwargs.items() if value != \"n/a\"}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "**Objective:**\n",
        "For the Term stated below, produce a realistic text passage that includes clearly identified named entities. Each entity should be meticulously labeled according to its type for straightforward extraction.\n",
        "\n",
        "**Term:**\n",
        "{term}\n",
        "\n",
        "**Definition of the Term:**\n",
        "{definition}\n",
        "\n",
        "**Entity Label**\n",
        "{predicted_label}\n",
        "\n",
        "**Format Requirements:**\n",
        "- The output should be formatted in JSON, containing the text and the corresponding entities list.\n",
        "- Ensure that none of the texts exceed 380 tokens.\n",
        "- Each entity in the text should be accurately marked and annotated in the \"entities\" list.\n",
        "- Meticulously follow all the listed attributes.\n",
        "- Do not include any additional entities other than the 'Term to Include' mentioned above.\n",
        "- Do not print \"json\" before the output, and do not include a code block in your response.\n",
        "\n",
        "**Entity Annotation Details:**\n",
        "- Entity types can be multiwords separated by space. For instance, use \"Entity Type\" rather than \"entity_type\".\n",
        "\n",
        "**Output Schema:**\n",
        "\n",
        "{{{{\n",
        "  \"text\": \"{{text content}}\",\n",
        "  \"entities\": [\n",
        "    {{\"entity\": \"entity name\", \"types\": [\"Type One\", \"Type Two\", ...]}},\n",
        "    ...\n",
        "  ]\n",
        "}}}}\n",
        "\n",
        "**Here are some real world examples**:\n",
        "{{{{\n",
        "\"text\": \"The burring machine in general use consists of the following parts: feed-sheet and rollers, revolving fan, lattice-sheet, revolving brush for passing the wool on to the surft, or cylinder; main cylinder, burr rollers, grid, and a large roller for beating the burrs on to the same; and, lastly, revolving brush for removing the wool off the cylinder. The tappet loom is one of the finest pieces of equipment available to the modern weaver\",\n",
        " \"entities\": [{{\"entity\": \"burring machine\",\n",
        "   \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"tappet loom\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"rollers\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"revolving fan\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"revolving brush\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"surft\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"cylinder\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"burr rollers\", \"types\": [\"Textile Machinery\"]}},\n",
        "  {{\"entity\": \"large roller\", \"types\": [\"Textile Machinery\"]}}]\n",
        "}}}}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    attributes_string = \" \".join([f'{key}=\"{value}\"' for key, value in attributes.items()])\n",
        "    prompt += f\"\\n<start {attributes_string}>\\n\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "all_prompts = []\n",
        "\n",
        "data = pd.read_csv('path/to/your/data.csv') # Replace with your actual data source\n",
        "\n",
        "NUM_EXAMPLES_PER_TERM = 3  # Define the number of examples per term\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in data.iterrows():\n",
        "    term = row['Term']\n",
        "    definition = row['Definition']\n",
        "    predicted_label = row['categories_1']  # Ensure the column name matches your CSV\n",
        "\n",
        "    # Generate multiple prompts per term\n",
        "    for _ in range(NUM_EXAMPLES_PER_TERM):\n",
        "        prompt = create_json_prompt_for_synthetic_data(\n",
        "            term=term,\n",
        "            definition=definition,\n",
        "            predicted_label=predicted_label,\n",
        "            language=\"english\",\n",
        "            types_of_text=\"descriptions of historic textile machinery and their use\"\n",
        "        )\n",
        "        all_prompts.append(prompt)\n",
        "\n",
        "NUM_SAMPLES = len(all_prompts)\n",
        "\n",
        "def generate_from_prompts(prompts, model=\"gpt-4o-mini\", temperature=0.7, max_workers=5):\n",
        "    all_outs = []\n",
        "\n",
        "    def process_prompt(prompt):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            output_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            output_text = output_text.strip()\n",
        "\n",
        "            if output_text.startswith(\"{\") and output_text.endswith(\"}\"):\n",
        "                js = json.loads(output_text)\n",
        "                return js\n",
        "            else:\n",
        "                print(\"Output does not look like JSON.\")\n",
        "                return None\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON decoding error: {e}\")\n",
        "            print(\"Response was:\", output_text)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Use ThreadPoolExecutor for concurrent API calls\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(process_prompt, prompt) for prompt in prompts]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result is not None:\n",
        "                all_outs.append(result)\n",
        "\n",
        "    return all_outs\n",
        "\n",
        "all_outs = generate_from_prompts(all_prompts)\n",
        "\n",
        "# Structure the results into a single JSON object\n",
        "results = {\n",
        "    \"results\": all_outs\n",
        "}\n",
        "\n",
        "json_results = json.dumps(results, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the results to a JSON file in your directory**\n",
        "\n"
      ],
      "metadata": {
        "id": "tgMhpv7uhEn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"path/to/your/output/results.json\", \"w\") as json_file: # Define your output directory\n",
        "    json_file.write(json_results)"
      ],
      "metadata": {
        "id": "AZ4A1zUsg_bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}